{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6665ce6-d4aa-40e4-8857-84bf8c3c4517",
   "metadata": {},
   "source": [
    "# Fit 7 & 8 class GMMs and calculate average profiles for profile matching\n",
    "\n",
    "This notebook will create the average profiles for 7 and 8 class GMMs fit to two ensembles from the UK-ESM historical simulations. These files are required to reproduce Figures 3 and 4 from *Heuristic Methods for Determining the Number of Classes in Unsupervised Classification of Climate Models*, E. Boland et al. 2022 (doi to follow). This requires cluster_utils.py and input datafiles via the googleapi CMIP6 store (see cluster_utils.py for more info)\n",
    "Outputs stored in model/\\[ensemble\\]/\\[nclasses\\]/avg.obj\n",
    "\n",
    "Please attribute any plots or code from this notebook using the DOI from Zenodo: to come\n",
    "\n",
    "Updated Nov 2022\n",
    "E Atkinson & E Boland [emmomp@bas.ac.uk](email:emmomp@bas.ac.uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60fff5b-9c55-4d9a-904e-998b7a31a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://127.0.0.1:39925\")\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03e3629-2adb-45a4-9b96-cf5a5c0b300c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "gateway = Gateway()\n",
    "from dask.distributed import Client\n",
    "\n",
    "'''properly shutdown any previous clusters'''\n",
    "clusters=gateway.list_clusters()\n",
    "if clusters != []:\n",
    "    print(f'found {len(clusters)} clusters')\n",
    "    for cluster in clusters:\n",
    "        cluster = gateway.connect(cluster.name)\n",
    "        client=Client(cluster)\n",
    "        client.close()\n",
    "        cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99523d4-09f4-4ac3-87d5-446097b0fc6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15f2aa-d8b9-4745-96c4-162f43dfd106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import cluster_utils as flt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8350b1e6-670b-4dd2-86a4-6e64d9cba187",
   "metadata": {},
   "source": [
    "### User options\n",
    "Leave as is to recreate the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7162e-e59a-4432-a503-746fefb9f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes \n",
    "classes = [7,8]\n",
    "model='model_20012017'\n",
    "#Time range\n",
    "tslice=slice('2001-01', '2017-12') \n",
    "ids = ['r1i1p1f2', 'r2i1p1f2'] \n",
    "levSel=slice(5, 2000)\n",
    "npca=3 #number of PCA components\n",
    "ntrain=3000 #number of profiles per month to use in training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a32ef0-fdc8-4c6a-8ace-9641e60e3f98",
   "metadata": {},
   "source": [
    "Uncomment the following two lines if you need to generate mask.npy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2f8d0-8da7-4a18-be38-4abd4ee7c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = flt.retrieve_profiles(timeRange = slice('1995-01', '1995-02'))\n",
    "#np.save('data/mask', data['n'])\n",
    "mask = np.load('data/mask_3000.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34bd54-429d-4756-a004-f0aad324152b",
   "metadata": {},
   "source": [
    "### Train models and generate average profiles for chosen ensemble members and classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fad51c55-05e3-4e19-8dda-653b96356ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting r1i1p1f2\n",
      "Finished setup for r1i1p1f2\n",
      "Classifying full dataset into 7 classes\n",
      "Classification complete, writing to file\n",
      "class written to model_20012017/r1i1p1f2/7/class.nc\n",
      "Average profiles calculated, writing to file\n",
      "Done with 7 classes\n",
      "Classifying full dataset into 8 classes\n",
      "Classification complete, writing to file\n",
      "class written to model_20012017/r1i1p1f2/8/class.nc\n",
      "Average profiles calculated, writing to file\n",
      "Done with 8 classes\n",
      "Starting r2i1p1f2\n",
      "Finished setup for r2i1p1f2\n",
      "Classifying full dataset into 7 classes\n",
      "Classification complete, writing to file\n",
      "class written to model_20012017/r2i1p1f2/7/class.nc\n",
      "Average profiles calculated, writing to file\n",
      "Done with 7 classes\n",
      "Classifying full dataset into 8 classes\n",
      "Classification complete, writing to file\n",
      "class written to model_20012017/r2i1p1f2/8/class.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-798' coro=<Client._gather.<locals>.wait() done, defined at /srv/conda/envs/notebook/lib/python3.9/site-packages/distributed/client.py:2002> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/distributed/client.py\", line 2011, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average profiles calculated, writing to file\n",
      "Done with 8 classes\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for m_id in ids:\n",
    "    \n",
    "    #Check if data already exists\n",
    "    tests=[]\n",
    "    for nn,n_classes in enumerate(classes):   \n",
    "        path_data = '{}/{}/{}'.format(model,m_id, n_classes)\n",
    "        tests.append(os.path.isfile('{}/avg_prof.obj'.format(path_data)))\n",
    "    if np.all(tests):\n",
    "        print('Found avg files for {}, skipping'.format(m_id))\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        print('Starting {}'.format(m_id))\n",
    "        options = {'memberId' : m_id}\n",
    "        path_id = '{}/{}'.format(model,m_id)\n",
    "        # Check if models already created\n",
    "        tests=[]\n",
    "        for nn,n_classes in enumerate(classes):   \n",
    "            path_data = '{}/{}/{}'.format(model,m_id, n_classes)\n",
    "            tests.append(os.path.isfile('{}/gmm.obj'.format(path_data)))\n",
    "        if np.all(tests): # All models trained, no need to load training set       \n",
    "            with open('{}/pca.obj'.format(path_id),'rb') as file:\n",
    "                pca=pickle.load(file)         \n",
    "        else:     # Load training set, generate PCA model\n",
    "            print('No models found, generating training set')\n",
    "            [data_train,pca] = flt.generate_trainingset(timeRange = tslice, mask=mask, options=options,n_components=npca,N=ntrain,levSel=levSel)\n",
    "            if not os.path.exists(path_id):\n",
    "                os.makedirs(path_id)\n",
    "            with open('{}/pca.obj'.format(path_id), 'wb') as file:\n",
    "                pickle.dump(pca, file)               \n",
    "                        \n",
    "        \n",
    "        # Retrieve all Southern Ocean data\n",
    "        options = {'memberId' : m_id}\n",
    "        data = flt.retrieve_profiles(timeRange=tslice,mask=mask,options=options,levSel=levSel)\n",
    "        data = data.chunk({'time': data.sizes['time'], 'n': 1024})\n",
    "        # Normalise the samples\n",
    "        data_norm = flt.normalise_data(data, ('n', 'time')) \n",
    "        # Transform to PCA space\n",
    "        data_trans = flt.pca_transform(data_norm, pca)\n",
    "        print('Finished setup for {}'.format(m_id))      \n",
    "\n",
    "        for nn,n_classes in enumerate(classes):  \n",
    "            path_n = '{}/{}/{}'.format(model,m_id, n_classes)\n",
    "            #Check if model already created\n",
    "            if os.path.isfile('{}/gmm.obj'.format(path_n)):\n",
    "                with open('{}/gmm.obj'.format(path_n),'rb') as file:\n",
    "                    gmm=pickle.load(file)                \n",
    "            else:            \n",
    "                print('Training {} class model'.format(n_classes))\n",
    "                if not os.path.exists(path_n):\n",
    "                    os.makedirs(path_data)            \n",
    "                gmm = flt.train_gmm(data_train, n_classes)\n",
    "                with open('{}/gmm.obj'.format(path_n), 'wb') as file:\n",
    "                    pickle.dump(gmm, file)\n",
    " \n",
    "            print('Classifying full dataset into {} classes'.format(n_classes))\n",
    "            # Classify full dataset\n",
    "            data_classes = flt.gmm_classify(data_trans, gmm)\n",
    "            print('Classification complete, writing to file'.format(n_classes))\n",
    "            flt.write_tonc(data_classes.reset_index('n'),n_classes,m_id,'class',path_n)     \n",
    "            # Calculate average profiles for each clasee\n",
    "            avg_prof = flt.avg_profiles(data, data_classes, n_classes)\n",
    "            print('Average profiles calculated, writing to file'.format(n_classes))\n",
    "            with open('{}/avg_prof.obj'.format(path_n), 'wb') as file:\n",
    "                pickle.dump(avg_prof, file)\n",
    "            print('Done with {} classes'.format(n_classes))\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e02e3-d223-4e95-9e77-744dc8cab1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
